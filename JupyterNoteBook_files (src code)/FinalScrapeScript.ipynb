{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74d8de2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "a78e7b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Authorized instance\n",
    "def getAuthenticated(c_id, c_secret, userAgent, userName, Pswd):\n",
    "    reddit = praw.Reddit(client_id= c_id,         # your client id\n",
    "                                    client_secret= c_secret,      # your client secret\n",
    "                                    user_agent= userAgent,        # your user agent\n",
    "                                    username= userName,        # your reddit username\n",
    "                                    password=Pswd\n",
    "                                  )\n",
    "    return reddit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "0a360548",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fayizaCre- Details1\n",
    "reddit = getAuthenticated(\"eTNNwDnVsEukX4z0AHvDYQ\", \n",
    "            \"e9EKeODCP_lksBhiBxSw397bbgd9xw\",   \n",
    "            \"HomelessDS\",        \n",
    "            \"Faii50301\",\n",
    "            \"cpsc@50301\")\n",
    "\n",
    "# choose the subreddit you want to retrieve data from\n",
    "subreddit = reddit.subreddit(\"homeless\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1cf9fe",
   "metadata": {},
   "source": [
    "The issue is likely due to the use of the last_id variable in the get_posts function, which is used to determine the next set of submissions to scrape. If the same last_id value is passed to the function each time it is called, it will result in the same set of submissions being retrieved repeatedly.\n",
    "\n",
    "To fix this, you should update the last_id variable within the while loop so that it reflects the most recent submission retrieved from Reddit. This will allow you to continuously retrieve new submissions, rather than the same set of submissions repeatedly.\n",
    "\n",
    "We have to check these:\n",
    "\n",
    "Check the value of upto_timestamp, if it's not decreasing with each iteration then there is a problem with the code logic.\n",
    "\n",
    "Verify that the get_posts function correctly returns the data frame with unique submissions. It could be that duplicates are being appended to all_posts_df each time.\n",
    "\n",
    "Verify the logic for breaking the loop when the upto_timestamp is less than 1577836800, it could be that the break condition is not being satisfied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "76a4e284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.geeksforgeeks.org/how-to-convert-timestamp-string-to-datetime-object-in-python/\n",
    "def dateANdTime(timestamp):\n",
    "    dt_obj = datetime.fromtimestamp(timestamp)\n",
    "    return dt_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "0c99632e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#most important\n",
    "def get_posts(n, last_id, collected_df):\n",
    "    posts_list = [] \n",
    "    \n",
    "    for submission in subreddit.controversial(limit=n):\n",
    "        \n",
    "        if (collected_df.empty == False):\n",
    "            \n",
    "            if(submission.id == collected_df['id'].iloc[0]): # if submission id has been collected then ignore this post\n",
    "                print(\"Test: just collected:\",submission.id ,\n",
    "                      \" = \", collected_df['id'].iloc[0] ,\n",
    "                      \" First id collected in previous batch \\n\" )\n",
    "                break\n",
    "        \n",
    "        \n",
    "        if(submission.author == None):   # if submission author is deleted then ignore this post\n",
    "            continue\n",
    "\n",
    "        info_list = []\n",
    "        info_list.append(submission.id)   # Output: the submission's title\n",
    "        info_list.append(submission.score)  # Output: the submission's score  \n",
    "        info_list.append((str(submission.author)))  # output: the name of the author\n",
    "        info_list.append(submission.num_comments) # output: the total number of comments made on this post\n",
    "        info_list.append(submission.subreddit)  # Output: subreddit where this submission was posted\n",
    "        info_list.append(submission.title)   # output: the title of the submission\n",
    "        info_list.append(submission.selftext) # output: the body of the submission\n",
    "        info_list.append(dateANdTime(submission.created_utc))  # output: the time stamp when this post was submitted\n",
    "\n",
    "        posts_list.append(info_list)   # append all the above information into this posts_list\n",
    "\n",
    "#     a = sorted(posts_list, key=lambda x: x[0], reverse = True)\n",
    "    posts_df = pd.DataFrame(posts_list, columns = ['id', 'score', 'author', 'num_comments', 'subreddit', 'post_title' , 'post_Content', 'timestamp'])\n",
    "\n",
    "    return posts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070666a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epoch timestamp: 1577836800\n",
    "# Timestamp in milliseconds: 1577836800000\n",
    "# Date and time (GMT): Wednesday, January 1, 2020 12:00:00 AM\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "fefc4dcb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\faii_\\AppData\\Local\\Temp\\ipykernel_19888\\3527990031.py:15: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  all_posts_df = all_posts_df.append(posts_df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "count:  1 in While loop \n",
      "\n",
      "          id  score                author  num_comments subreddit  \\\n",
      "0    106dh85      0            Gollumborn            33  homeless   \n",
      "1     xpbzvf      0               treyj88            31  homeless   \n",
      "2     p3zift      0              _iillii_            28  homeless   \n",
      "3     wn4hzf      0         LALakersFan24            23  homeless   \n",
      "4     xtxumd      0   Aggressive_Lion_587            19  homeless   \n",
      "..       ...    ...                   ...           ...       ...   \n",
      "875   ik773y      7      HauntedCrocodile             6  homeless   \n",
      "876   gqqnak      8  AndrewEldritchHorror            22  homeless   \n",
      "877   f7ovi5      5             myco-naut             6  homeless   \n",
      "878   eyh3a6      6             zodiac707            32  homeless   \n",
      "879   elj184      7             R0ntastic            10  homeless   \n",
      "\n",
      "                                            post_title  \\\n",
      "0                                 Why all the garbage?   \n",
      "1                       A free ticket to a better life   \n",
      "2                     JessieDaMess banned from the sub   \n",
      "3    Just Found This YouTube Channel of This Homele...   \n",
      "4    I can no longer engage in any commerce,transac...   \n",
      "..                                                 ...   \n",
      "875                                Care Package Ideas?   \n",
      "876  I anticipate being homeless in my mid-life thi...   \n",
      "877  I'm going downtown Denver tonight. Got some ca...   \n",
      "878            Best State to Start Over(with nothing)?   \n",
      "879                  Homelessness = legalized homicide   \n",
      "\n",
      "                                          post_Content           timestamp  \n",
      "0    In my small city, we were glad to help our hom... 2023-01-08 00:53:36  \n",
      "1    Would anybody like to read a story? Iâ€™m assumi... 2022-09-27 03:38:32  \n",
      "2    I just learned from Jessie that she was banned... 2021-08-13 20:21:08  \n",
      "3                                                      2022-08-12 21:43:05  \n",
      "4    I figured id find advice and help here. I can,... 2022-10-02 13:44:29  \n",
      "..                                                 ...                 ...  \n",
      "875  I want to create care packages to keep in my c... 2020-08-31 15:37:10  \n",
      "876  I can't and do not want to reintegrate into so... 2020-05-25 22:59:00  \n",
      "877  It's a $50 night and 3 g's of live resin to sm... 2020-02-21 23:35:51  \n",
      "878  I am a 38 y/o White Male living with my boomer... 2020-02-03 17:27:11  \n",
      "879  Homelessness looks like society's way of Carry... 2020-01-07 15:34:54  \n",
      "\n",
      "[880 rows x 8 columns]\n",
      "\n",
      "last_id:  elj184\n",
      "last_timestamp 2020-01-07 15:34:54 \n",
      "\n",
      "Test: just collected: 106dh85  =  106dh85  First id collected in previous batch \n",
      "\n",
      "\n",
      "count:  2 in While loop \n",
      "\n",
      "Empty DataFrame\n",
      "Columns: [id, score, author, num_comments, subreddit, post_title, post_Content, timestamp]\n",
      "Index: []\n",
      "\n",
      "reach end!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\faii_\\AppData\\Local\\Temp\\ipykernel_19888\\3527990031.py:15: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  all_posts_df = all_posts_df.append(posts_df)\n"
     ]
    }
   ],
   "source": [
    "#most important\n",
    "\n",
    "all_posts_df = pd.DataFrame(columns = ['id', 'score', 'author', 'num_comments',\n",
    "                                       'subreddit', 'post_title' , 'post_Content', 'timestamp'])\n",
    "\n",
    "\n",
    "limit = None\n",
    "last_id = None\n",
    "count = 0\n",
    "\n",
    "while True:\n",
    "    \n",
    "    count +=1\n",
    "    posts_df = get_posts(limit, last_id, all_posts_df) # get submission(post) - recursively\n",
    "    all_posts_df = all_posts_df.append(posts_df)\n",
    "    \n",
    "    print(\"\\ncount: \" , count, \"in While loop \\n\")\n",
    "    print(posts_df)\n",
    "    \n",
    "    if len(posts_df) <= 0:\n",
    "#     if upto_timestamp <= 1577836800: # getting posts upto this time Wednesday, January 1, 2020 12:00:00 AM\n",
    "        print(\"\\nreach end!!!\")\n",
    "        break\n",
    "        \n",
    "    last_id = posts_df['id'].iloc[-1]  # getting last post's id from dataframe\n",
    "    upto_timestamp = posts_df.iloc[-1]['timestamp']\n",
    "    \n",
    "\n",
    "    print(\"\\nlast_id: \", last_id)\n",
    "    print(\"last_timestamp\", upto_timestamp, \"\\n\")\n",
    "    \n",
    "    \n",
    " \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    #     #check statements\n",
    "# #     ctime= datetime.now()\n",
    "#      print(\"before: \", ctime.strftime(\"%H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "f8ae217c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Unique_Values(repeating, df_columnName):\n",
    "    unique_users_df = repeating.drop_duplicates(subset= df_columnName)\n",
    "    return unique_users_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "fb54894c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "646\n",
      "933\n"
     ]
    }
   ],
   "source": [
    "newPost_nodes_df = get_Unique_Values(all_posts_df, 'author')\n",
    "newPost_post_uniq = get_Unique_Values(all_posts_df, 'id')\n",
    "print(len(newPost_nodes_df))\n",
    "print(len(newPost_post_uniq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "09a3069f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "496\n",
      "717\n"
     ]
    }
   ],
   "source": [
    "hotPost_nodes_df = get_Unique_Values(all_posts_df, 'author')\n",
    "hotPost_post_uniq = get_Unique_Values(all_posts_df, 'id')\n",
    "print(len(hotPost_nodes_df))\n",
    "print(len(hotPost_post_uniq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "45108513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "717\n"
     ]
    }
   ],
   "source": [
    "topPost_post_uniq = get_Unique_Values(all_posts_df, 'id')\n",
    "print(len(hotPost_post_uniq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "356ffacc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "880\n"
     ]
    }
   ],
   "source": [
    "controversialPost_post_uniq = get_Unique_Values(all_posts_df, 'id')\n",
    "print(len(controversialPost_post_uniq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "eff305f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\faii_\\AppData\\Local\\Temp\\ipykernel_19888\\2533395087.py:4: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  collection_df = collection_df.append(newPost_post_uniq)\n",
      "C:\\Users\\faii_\\AppData\\Local\\Temp\\ipykernel_19888\\2533395087.py:5: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  collection_df = collection_df.append(hotPost_post_uniq)\n",
      "C:\\Users\\faii_\\AppData\\Local\\Temp\\ipykernel_19888\\2533395087.py:6: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  collection_df = collection_df.append(topPost_post_uniq)\n",
      "C:\\Users\\faii_\\AppData\\Local\\Temp\\ipykernel_19888\\2533395087.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  collection_df = collection_df.append(controversialPost_post_uniq)\n"
     ]
    }
   ],
   "source": [
    "collection_df = pd.DataFrame(columns = ['id', 'score', 'author', 'num_comments',\n",
    "                                       'subreddit', 'post_title' , 'post_Content', 'timestamp'])\n",
    "\n",
    "collection_df = collection_df.append(newPost_post_uniq)\n",
    "collection_df = collection_df.append(hotPost_post_uniq)\n",
    "collection_df = collection_df.append(topPost_post_uniq)\n",
    "collection_df = collection_df.append(controversialPost_post_uniq)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f30f5c92",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_Unique_Values' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m post_check \u001b[38;5;241m=\u001b[39m \u001b[43mget_Unique_Values\u001b[49m(collection_df, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m author_check \u001b[38;5;241m=\u001b[39m get_Unique_Values(collection_df, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauthor\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(post_check))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'get_Unique_Values' is not defined"
     ]
    }
   ],
   "source": [
    "post_check = get_Unique_Values(collection_df, 'id')\n",
    "author_check = get_Unique_Values(collection_df, 'author')\n",
    "print(len(post_check))\n",
    "print(len(author_check))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "38194702",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_df.to_csv('.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "7b5f2c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_check.to_csv('newhotTopConPostsData.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b256c99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_posts = pd.read_csv('newhotTopConPostsData.csv')\n",
    "\n",
    "\n",
    "all_posts['timestamp'] = pd.to_datetime(all_posts['timestamp'])\n",
    "\n",
    "# Split the 'timestamp' column into separate columns for year, month, day, hour, minute, and second\n",
    "all_posts['year'], all_posts['month'], all_posts['day'], all_posts['hour'], all_posts['minute'], all_posts['second'] = all_posts['timestamp'].dt.year, all_posts['timestamp'].dt.month, all_posts['timestamp'].dt.day, all_posts['timestamp'].dt.hour, all_posts['timestamp'].dt.minute, all_posts['timestamp'].dt.second\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3d3191d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_posts = all_posts.sort_values(by=['year', 'month', 'day'], ascending=[False, False, False])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e79cf8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2019_later = all_posts.query('year > 2019')\n",
    "\n",
    "df_before_2019 = all_posts.query('year <= 2019')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "44dab3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = np.array_split(df_2019_later, 3)\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    chunk.to_csv(f'all_posts_2019_later{i}.csv', index=False)\n",
    "    \n",
    "df_before_2019.to_csv(\"all_posts_before_2019.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
